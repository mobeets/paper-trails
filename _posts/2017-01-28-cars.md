---
layout: post
title: "Week #4: Self-driving cars"
number: 4
---

__Paper:__ [_End-to-end Learning of Driving Models from Large-scale Video Datasets_](https://arxiv.org/abs/1612.01079), by Huazhe Xu et al. (_arXiv_, 2016) [[pdf]](https://arxiv.org/pdf/1612.01079.pdf).

From one perspective, the task of making a self-driving car sounds insanely difficult: The car must avoid hitting other cars and pedestrians, stay in its lane, obey the laws, and handle a few [ethical dilemmas](http://science.sciencemag.org/content/352/6293/1573)&mdash;all of this while navigating you quickly to your destination.

But from another perspective, an autonomous car's job is very simple: All it has to do is decide how and when to turn the steering wheel, and how and when to press the brake and accelerator. That's a whole lot more complicated than, say, becoming a professional tennis player.

This week's paper takes the latter perspective, and is all about teaching an autonomous car how to drive in an "end-to-end" fashion. This is the idea that, __provided we give our car enough sensors, the car can simply learn the relationship between what it sees and how it turns the wheel and presses the brakes and gas.__ We don't have to teach it explicitly about lanes or other cars or any of that. Sounds simple, right?

(Obviously, saying that driving a car is just a matter of turning the wheel and pressing the pedals is kinda like saying that being a chess master is simply a matter of moving chess pieces. On the other hand, given that the world's [top chess _and_ Go player](https://arxiv.org/abs/1712.01815) learned how to play without any human instruction, maybe there's some value in seeing how far we can get solely by training an algorithm rather than by having it mimic what humans do.)

## How to make a self-driving car

In this paper, the authors develop a neural network model that, at each point in time, translates an image from a video feed into a suitable action. The actions the authors consider here are "straight", "stop", "left turn", and "right turn". (Driving a car well turns out to be difficult enough even in this simplified setting.)

You can think of this model as a function, called _f_. The function is trained to learn how to map an image, _x_, to the action, _a_, it believes is most reasonable: a = f(x).

If you're not familiar with neural networks, when I say "function" I really do mean something like the f(x) = mx+b stuff you learned in school. Only here, instead of _x_ being a single number, _x_ is an image, which is represented as a list of numbers corresponding to the colors of every single pixel in the image. And instead of _f_ being a linear function (as in mx+b), instead we have something like [this](http://neuralnetworksanddeeplearning.com/chap1.html#sigmoid_neurons) (see equation 4). Finally, computer scientists like catchy names, so instead of saying "fitting a curve to a bunch of data points" they call it "training." The A.I. apocalypse is upon us!

The trick here is that in order to train these networks, you need a lot of training data covering a whole lot of different driving conditions. So the authors use a data set they curated called the Berkeley DeepDrive Video dataset ([BDDV](http://data-bdd.berkeley.edu/)), which comprises roughly 10,000 hours of video footage (e.g., from a dashcam) of cars driving in different cities, during different weather conditions, and so on. The largest prior data set consisted of only 214 hours of driving, so this is a huge improvement.

The data also includes GPS, gyroscope, [IMU](https://en.wikipedia.org/wiki/Inertial_measurement_unit), and [magnetometer](https://en.wikipedia.org/wiki/Magnetometer) measurements, which they can use to figure out what the driver in the videos did at each moment in time. So now, given an image frame from the camera, the goal is that the network's suggested action is the same as whatever the human in the video did during the next frame of the video.

## Most self-driving cars are hand-crafted algorithms

Given the hype around both self-driving cars and machine learning, I was pretty surprised to learn that __neural networks [are not](https://arxiv.org/abs/1504.01716) what decides how autononmous cars move around.__

Instead, neural networks are used merely to detect assistive information&mdash;things like where the other cars are, where the center of the lane is, etc.

First there's the issue of sensors. So you give your vehicle a GPS, a camera, an accelerometer, radar, sonar, LIDAR, etc.

Now the task is a little clearer: Given only the information from your sensors, decide whether to turn the wheel, press the brake, etc.

Weaknesses of approach:
- The car has no idea where it is or where it's going (i.e., this is not SLAM)
- The goal here is, given an image, predict left vs. right. This will keep you in your lane and on the road but it won't take you to your destination.

- goal is end-to-end learning of how to drive
	- proposed modification: learning _how to stay in your lane_

Common approach: SLAM, or simultaneous localization and mapping

- https://en.wikipedia.org/wiki/TORCS
	- "In December 2000 CNN placed TORCS among the 'Top 10 Linux games for the holidays'."

## Interesting facts

1. long-range car detection requires radar, while nearby car detection requires sonar
	- radar can register tin cans as cars
	- cameras are cheaper, but historically, require much finer tuning and more human intervention
		- because the world is so varied, visually...richer data -> harder learning problem
2. DARPA: In the first year (2004), no teams completed the race; in 2005, five teams did, and 28 teams made it further than any car did in 2004
	- however, in the DARPA challenges, teams are given maps of the route ahead of time, including stop signs, etc.

3. Some autonomous vacuums such as the Roomba 980 use SLAM: [video](https://www.youtube.com/watch?v=oj3Vawn-kRE)

Links:

- Convert your current car to be self-driving for ~$2000: [link](http://x-matik.com/)
- Evil cars in movies: [link](https://flashbak.com/road-rage-the-5-most-evil-vehicles-in-movie-history-17875/)
- team notes from the 2007 Darpa Urban Challenge: [pdf](https://web.archive.org/web/20150211202449/http://teamjefferson.com/wp-content/uploads/2014/06/Debrief-Complete-TeamJefferson-DRAFT.pdf)
- SLAM tutorial: [link](http://sci-hub.tw/http://ieeexplore.ieee.org/abstract/document/1638022/)
