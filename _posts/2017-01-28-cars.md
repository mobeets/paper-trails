---
layout: post
title: "Week #4: Self-driving cars"
number: 4
---

__Paper:__ [_End-to-end Learning of Driving Models from Large-scale Video Datasets_](https://arxiv.org/abs/1612.01079), by Huazhe Xu et al. (_arXiv_, 2016) [[pdf]](https://arxiv.org/pdf/1612.01079.pdf).

I was sitting in class last week and my professor mentioned the above paper with special affection. Given how often self-driving cars have been in the news lately, I figured this might be a good paper to get a feel for how these things work.

When I hear "self-driving cars," I don't know about you, but I immediately think about A.I. and machine learning. But I was pretty surprised to learn that __neural networks [are not](https://arxiv.org/abs/1504.01716) critically involved in deciding how autonomous cars move around.__

Instead, neural networks are more commonly used to detect assistive information&mdash;things like where the other cars are, where the center of the lane is, etc. But even in this arena, traditional engineering approaches are more common than neural networks. So keep that in mind when we get to this week's paper, which takes a pretty modest approach.

### Self-driving cars and their sensors

There are two key parts involved in making an autonomous car: First, you need actuators, which are what translates the signals from the computer into actual physical control of your steering wheel, and the gas and brake pedals. (__Here's a terrifying idea: [$1500](http://x-matik.com/) is all it takes to turn your normal car into a self-driving car.__)

Second, you need a bunch of sensors. I mean, a _lot_ of sensors. Uber's self-driving Volvos, for example, [have](http://robotsforroboticists.com/sensors-uber-self-driving-car/):

- 20 cameras
- 4 or so GPS devices
- sonar (good for detecting cars at close range)
- radar (used for long-range car detection, but are easily fooled by tin cans)
- intertial measurement unit, or IMU (tells you the relative orientation of the car itself)
- multiple lidar devices

If you've ever seen a self-driving car, you might have noticed a giant spinning thing on the top of it. [Apparently](http://www.detroitnews.com/story/business/autos/mobility/2017/09/07/self-driving-uber-pittsburgh/105375396/) Uber engineers refer to it as "the chicken bucket," because it looks like a giant, spinning, futuristic bucket of KFC chicken.

The bucket is actually a lidar device, and it seems to be one of the core sensors of most autonomous cars. Lidar gets its name from LIght plus raDAR, and it's used for measuring the depths of the objects around it. For a simple lidar system, all you need is a laser, a mirror, and a sensor ([gif](https://en.wikipedia.org/wiki/Lidar#/media/File:LIDAR-scanned-SICK-LMS-animation.gif)). [More complicated options](http://velodynelidar.com/hdl-64e.html) gather millions of measurements per second.

### Stanley and Roombas

I mentioned that most autonomous cars don't use neural networks to do the actual driving. Well, it turns out they often don't do the computer vision parts either.

Instead, the data captured by all your sensors is processed by a suite of algorithms referred to broadly as [SLAM](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping), which stands for _simultaneous localization and mapping_. Everyone uses SLAM. [Stanley](https://en.wikipedia.org/wiki/Stanley_(vehicle)), the first winner of the DARPA Grand Challenge used SLAM. Even the [new Roomba vacuums]([video](https://www.youtube.com/watch?v=oj3Vawn-kRE)) use SLAM. The task is simple enough to describe: As your robot navigates the world, it has no idea about its environment _or_ where it is in that environment. So the goal of SLAM is to build up a map of your surroundings as well as where you are in that environment.

In case you hadn't heard of it, the [DARPA Grand Challenge](https://en.wikipedia.org/wiki/DARPA_Grand_Challenge) was a competition set up to sponsor the development of autonomous vechicles. To win the $2 million prize, your autonomous vehicle had to navigate a 7 mile course in the desert, totally unassisted. In 2004, the first year of the challenge, not a single team competed the race. _The very next year_, not only did five teams complete the race, but 22 of the 23 competitors got further than any other car did the previous year. That's how fast this technology came to fruition.

So, now we know that neural networks are not really required for self-driving cars at all. And from the papers I've seen on this topic ([here](https://arxiv.org/abs/1504.01716) and [here](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Chen_DeepDriving_Learning_Affordance_ICCV_2015_paper.html), for example), the use of neural networks in self-driving cars is very much in its infancy. I think the approach in this week's paper will show you what I mean.

### Driving is easy

From one perspective, the task of making a self-driving car sounds insanely difficult: The car must avoid hitting other cars and pedestrians, stay in its lane, obey the laws, and handle a few [ethical dilemmas](http://science.sciencemag.org/content/352/6293/1573)&mdash;all of this while navigating a passenger quickly to their destination.

But from another perspective, an autonomous car's job is very simple: All it has to do is decide how and when to turn the steering wheel, and how and when to press the brake and accelerator. Put it that way, and it sounds a whole lot less complicated than, say, becoming a professional tennis player.

This week's paper takes this perspective, and is all about teaching an autonomous car how to drive in an "end-to-end" fashion. This is the idea that, provided we give our car enough sensors, __the system should be able to learn all on its own the relationship between what it sees and how it should turn the wheel and press the brakes and gas.__ We don't have to teach it explicitly about lanes or other cars or any of that. Sounds easy, right?

In this paper, the authors develop a neural network model that, at each point in time, translates an image from a video feed into a suitable action. __The only actions the authors consider here are "straight", "stop", "left turn", and "right turn".__ Driving a car well turns out to be difficult enough even in this simplified setting.

You can think of this model as a function, called _f_. The function is trained to map an image, _x_, to the action, _a_, it believes is most reasonable: a = f(x).

If you're not familiar with neural networks, when I say "function" I really do mean something like the f(x) = mx+b stuff you learned in school. Only here, instead of _x_ being a single number, _x_ is an image, which is represented as a list of numbers corresponding to the colors of every single pixel in the image. And instead of _f_ being a linear function (as in mx+b), here we have a bunch of functions like [this](http://neuralnetworksanddeeplearning.com/chap1.html#sigmoid_neurons) (see equation 4) combined together. Finally, computer scientists like catchy names, so instead of saying "fitting a curve to a bunch of data points" they call it "training." The A.I. apocalypse is upon us!

The trick is that in order to train these networks, you need a lot of training data. So the authors introduce a data set they curated called the Berkeley DeepDrive Video dataset ([BDDV](http://data-bdd.berkeley.edu/)), which comprises roughly 10,000 hours of dashcam video footage of cars driving in different cities, during different weather conditions, and so on. The largest data set before this one consisted of only 214 hours of driving, so this is a huge improvement.

The data set also includes additional measurements that can be used to figure out what the drivers in the videos did at each moment in time. So now, if we give an image to the network, we can compare the network's suggested action to what the human in the video did in the same situation.

Notably, __the model has absolutely no idea where the drivers in the videos are headed.__ It turns out that, even despite this complete ignorance of where it is in the world and where it's going, its predictions are still about 84% accurate. Presumably that's because most of the time, all we do as drivers is stay in our lanes  and follow the road. On the other hand, you can get to 80% accuracy from knowing the car's speed alone (i.e., no neural network needed). Given that, adding an extra 4% doesn't seem like much of an accomplishment for their neural network.

So, in the end, I'm going to chalk up my professor's fondness for this paper to its way of framing the problem: It made self-driving cars sound so _easy_.

### Summary

- _Topic_: Translating images from a dashcam into driving instructions like "turn", "stop", etc.
- _Why we care_: Current autonomous vehicles incorporate rule-based driving policies, whereas learning-based policies might be better suited for handling complex or rare scenarios
- _Conclusion_: The authors use an extremely large data set of dashcam videos depicting cars driving in a variety of different conditions, along with a novel neural network model, to learn a mapping between images and driving instructions

### Related links

- SLAM tutorial: [link](http://sci-hub.tw/http://ieeexplore.ieee.org/abstract/document/1638022/)
- Evil cars in movies: [link](https://flashbak.com/road-rage-the-5-most-evil-vehicles-in-movie-history-17875/)
- Team notes from the 2007 Darpa Urban Challenge: [pdf](https://web.archive.org/web/20150211202449/http://teamjefferson.com/wp-content/uploads/2014/06/Debrief-Complete-TeamJefferson-DRAFT.pdf)
- There is a driving simulator called [TORCS](https://en.wikipedia.org/wiki/TORCS) that is used in a lot of research on autonomous cars. Wikipedia tells me that "In December 2000 CNN placed TORCS among the 'Top 10 Linux games for the holidays'."
